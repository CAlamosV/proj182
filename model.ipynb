{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import Tensor\n",
    "from data_generation import *\n",
    "from torchvision import transforms\n",
    "from PIL import Image, ImageDraw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence, positions = generate_random_sequence()\n",
    "display_sequence(sequence)\n",
    "print(positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "N = 1000  # Number of sequences\n",
    "L = 10  # Length of each sequence\n",
    "H, W = 16, 16  # Dimensions of the images\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "num_epochs = 20\n",
    "\n",
    "\n",
    "# Data Generation\n",
    "class PixelDataset(Dataset):\n",
    "    def __init__(self, num_sequences, sequence_length):\n",
    "        self.data = []\n",
    "        self.targets = []\n",
    "        for _ in range(num_sequences):\n",
    "            images, positions = generate_random_sequence()\n",
    "            self.data.append(images)  # Store the whole sequence\n",
    "            self.targets.append(positions)  # Store all positions\n",
    "\n",
    "        self.transform = transforms.ToTensor()\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Transform and stack images to create a sequence\n",
    "        sequence_of_images = [self.transform(image) for image in self.data[idx]]\n",
    "        sequence_of_images = torch.stack(sequence_of_images)\n",
    "\n",
    "        # Stack target positions\n",
    "        target_positions = torch.FloatTensor(self.targets[idx])\n",
    "\n",
    "        return sequence_of_images, target_positions\n",
    "\n",
    "\n",
    "# Creating Dataset and DataLoader\n",
    "dataset = PixelDataset(N, L)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-np.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[: x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTransformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleTransformer, self).__init__()\n",
    "        self.encoder = nn.TransformerEncoderLayer(d_model=H * W, nhead=4)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder, num_layers=3)\n",
    "        self.fc = nn.Linear(H * W, 2)  # Output is x, y position\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Flatten the image and encode\n",
    "        batch_size, sequence_length, _, _, _ = x.size()\n",
    "\n",
    "        # 2D positional encoding\n",
    "        # https://static.us.edusercontent.com/files/qjovJzQ2ERbedQv2ZeCZKAPS\n",
    "\n",
    "        x = x.view(batch_size, sequence_length, -1)\n",
    "        encoded = self.transformer_encoder(x)\n",
    "        output = self.fc(encoded)  # Apply the linear layer to the entire sequence\n",
    "        return output\n",
    "\n",
    "\n",
    "def euclidean_distance_loss(output, target):\n",
    "    epsilon = 1e-6  # A small constant to avoid sqrt(0)\n",
    "    return torch.sqrt(torch.sum((output - target) ** 2, dim=-1) + epsilon).mean()\n",
    "\n",
    "\n",
    "model = SimpleTransformer()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = euclidean_distance_loss\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for images, positions in dataloader:\n",
    "        # Shift the positions by one to create the targets\n",
    "        targets = torch.roll(positions, -1, dims=1)\n",
    "\n",
    "        # Mask the last position in each sequence as it has no valid next position\n",
    "        mask = torch.zeros_like(targets)\n",
    "        mask[:, :-1, :] = 1  # Mask all but the last position\n",
    "\n",
    "        # Forward pass\n",
    "        predicted_positions = model(images)\n",
    "\n",
    "        # Apply the mask\n",
    "        masked_predicted = predicted_positions * mask\n",
    "        masked_targets = targets * mask\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(masked_predicted, masked_targets)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sequence(model, sequence_of_images, device):\n",
    "    \"\"\"\n",
    "    Predict the positions for a given sequence of images.\n",
    "\n",
    "    :param model: Trained PyTorch model.\n",
    "    :param sequence_of_images: A sequence of images as a PyTorch tensor.\n",
    "    :param device: Device to perform computations on (e.g., 'cuda', 'cpu').\n",
    "    :return: Predicted positions as a tensor.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        sequence_of_images = sequence_of_images.to(device)\n",
    "        predicted_positions = model(sequence_of_images)\n",
    "    return predicted_positions.cpu()\n",
    "\n",
    "\n",
    "def create_images_from_positions(positions, image_size=(16, 16)):\n",
    "    \"\"\"\n",
    "    Create a sequence of PIL images from a sequence of position tuples.\n",
    "    Includes an empty image at the beginning of the sequence.\n",
    "\n",
    "    :param positions: A sequence of (x, y) tuples as a PyTorch tensor.\n",
    "    :param image_size: Size of the output images (width, height).\n",
    "    :return: A list of PIL.Image.Image objects.\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    positions = positions.squeeze()\n",
    "    if positions.dim() == 1:\n",
    "        positions = positions.unsqueeze(0)\n",
    "\n",
    "    # Create and add an empty image for the first position\n",
    "    empty_img = Image.new(\"1\", image_size, 255)  # White image\n",
    "    images.append(empty_img)\n",
    "\n",
    "    for pos in positions:\n",
    "        img = Image.new(\"1\", image_size, 255)  # Create a white image\n",
    "        draw = ImageDraw.Draw(img)\n",
    "        x, y = pos.tolist()\n",
    "        y = image_size[1] - 1 - y  # Adjust y-coordinate\n",
    "        draw.point((x, y), fill=0)  # Draw the black pixel\n",
    "        images.append(img)\n",
    "\n",
    "    return images\n",
    "\n",
    "\n",
    "def prepare_sequence_for_model(image_sequence):\n",
    "    \"\"\"\n",
    "    Prepare a sequence of PIL images for input into the model.\n",
    "\n",
    "    :param image_sequence: A list of PIL.Image.Image objects.\n",
    "    :return: A PyTorch tensor representing the sequence.\n",
    "    \"\"\"\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Grayscale(),  # Convert to grayscale if not already\n",
    "            transforms.ToTensor(),  # Convert PIL Image to PyTorch tensor\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    tensor_sequence = [transform(img) for img in image_sequence]\n",
    "    tensor_sequence = torch.stack(tensor_sequence)  # Stack to create a single tensor\n",
    "    tensor_sequence = tensor_sequence.unsqueeze(0)  # Add a batch dimension\n",
    "\n",
    "    return tensor_sequence\n",
    "\n",
    "\n",
    "def overlay_predicted_positions(s1, s2):\n",
    "    \"\"\"\n",
    "    s1 black pixels stay black\n",
    "    s2 black pixels become blue\n",
    "\n",
    "    :param s1: A list of PIL.Image.Image objects representing the original sequence.\n",
    "    :param s2: A list of PIL.Image.Image objects representing the predicted sequence.\n",
    "    :return: A list of PIL.Image.Image objects with overlaid positions.\n",
    "    \"\"\"\n",
    "    overlaid_images = []\n",
    "    for img1, img2 in zip(s1, s2):\n",
    "        img1 = img1.convert(\"RGB\")  # Convert to RGB\n",
    "        img2 = img2.convert(\"RGB\")\n",
    "\n",
    "        # Create a new image for the overlay\n",
    "        new_img = Image.new(\"RGB\", img1.size)\n",
    "        pixels1 = img1.load()\n",
    "        pixels2 = img2.load()\n",
    "\n",
    "        for x in range(img1.width):\n",
    "            for y in range(img1.height):\n",
    "                if pixels2[x, y] == (0, 0, 0):  # Black pixel in s2\n",
    "                    new_img.putpixel((x, y), (0, 0, 255))  # Make it blue\n",
    "                else:\n",
    "                    new_img.putpixel((x, y), pixels1[x, y])  # Use pixel from s1\n",
    "\n",
    "        overlaid_images.append(new_img)\n",
    "\n",
    "    return overlaid_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence, positions = generate_random_sequence()\n",
    "sequence_of_images = prepare_sequence_for_model(sequence)\n",
    "predicted_positions = predict_sequence(\n",
    "    model, sequence_of_images, device=\"cpu\"\n",
    ")  # or 'cuda'\n",
    "predicted_positions = predicted_positions[:, 1:, :]\n",
    "predicted_images = create_images_from_positions(predicted_positions)\n",
    "display_sequence(overlay_predicted_positions(sequence, predicted_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
