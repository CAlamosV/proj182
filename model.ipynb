{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from data_generation import *\n",
    "from torchvision import transforms\n",
    "from PIL import Image, ImageDraw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x200 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(6.118723977510821, 5.050649633235524), (5.677215560876097, 1.6532339699126974), (5.235707144241373, 1.9665955923149168), (4.794198727606649, 1.4985884051982148), (4.3526903109719255, 1.498785460924016), (3.9111818943372016, 1.498785460924016), (3.469673477702478, 1.498785460924016), (3.028165061067754, 1.498785460924016), (2.58665664443303, 1.498785460924016), (2.1451482277983063, 1.498785460924016)]\n"
     ]
    }
   ],
   "source": [
    "sequence, positions = generate_random_sequence()\n",
    "display_sequence(sequence)\n",
    "print(positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "N = 1000  # Number of sequences\n",
    "L = 10  # Length of each sequence\n",
    "H, W = 16, 16  # Dimensions of the images\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "num_epochs = 20\n",
    "\n",
    "\n",
    "# Data Generation\n",
    "class PixelDataset(Dataset):\n",
    "    def __init__(self, num_sequences, sequence_length):\n",
    "        self.data = []\n",
    "        self.targets = []\n",
    "        for _ in range(num_sequences):\n",
    "            images, positions = generate_random_sequence()\n",
    "            self.data.append(images)  # Store the whole sequence\n",
    "            self.targets.append(positions)  # Store all positions\n",
    "\n",
    "        self.transform = transforms.ToTensor()\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Transform and stack images to create a sequence\n",
    "        sequence_of_images = [self.transform(image) for image in self.data[idx]]\n",
    "        sequence_of_images = torch.stack(sequence_of_images)\n",
    "\n",
    "        # Stack target positions\n",
    "        target_positions = torch.FloatTensor(self.targets[idx])\n",
    "\n",
    "        return sequence_of_images, target_positions\n",
    "\n",
    "\n",
    "# Creating Dataset and DataLoader\n",
    "dataset = PixelDataset(N, L)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 2.4393\n",
      "Epoch [2/20], Loss: 2.0480\n",
      "Epoch [3/20], Loss: 1.0963\n",
      "Epoch [4/20], Loss: 1.7283\n",
      "Epoch [5/20], Loss: 0.8799\n",
      "Epoch [6/20], Loss: 1.4094\n",
      "Epoch [7/20], Loss: 0.9298\n",
      "Epoch [8/20], Loss: 1.2095\n",
      "Epoch [9/20], Loss: 0.9576\n",
      "Epoch [10/20], Loss: 1.0218\n",
      "Epoch [11/20], Loss: 0.4336\n",
      "Epoch [12/20], Loss: 0.6249\n",
      "Epoch [13/20], Loss: 0.6877\n",
      "Epoch [14/20], Loss: 0.7807\n",
      "Epoch [15/20], Loss: 0.6369\n",
      "Epoch [16/20], Loss: 0.7651\n",
      "Epoch [17/20], Loss: 0.5060\n",
      "Epoch [18/20], Loss: 0.6236\n",
      "Epoch [19/20], Loss: 0.5601\n",
      "Epoch [20/20], Loss: 0.5454\n"
     ]
    }
   ],
   "source": [
    "class SimpleTransformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleTransformer, self).__init__()\n",
    "        self.encoder = nn.TransformerEncoderLayer(d_model=H * W, nhead=4)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder, num_layers=3)\n",
    "        self.fc = nn.Linear(H * W, 2)  # Output is x, y position\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Flatten the image and encode\n",
    "        batch_size, sequence_length, _, _, _ = x.size()\n",
    "        x = x.view(batch_size, sequence_length, -1)\n",
    "        encoded = self.transformer_encoder(x)\n",
    "        output = self.fc(encoded)  # Apply the linear layer to the entire sequence\n",
    "        return output\n",
    "\n",
    "\n",
    "def euclidean_distance_loss(output, target):\n",
    "    epsilon = 1e-6  # A small constant to avoid sqrt(0)\n",
    "    return torch.sqrt(torch.sum((output - target) ** 2, dim=-1) + epsilon).mean()\n",
    "\n",
    "\n",
    "model = SimpleTransformer()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = euclidean_distance_loss\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for images, positions in dataloader:\n",
    "        # Shift the positions by one to create the targets\n",
    "        targets = torch.roll(positions, -1, dims=1)\n",
    "\n",
    "        # Mask the last position in each sequence as it has no valid next position\n",
    "        mask = torch.zeros_like(targets)\n",
    "        mask[:, :-1, :] = 1  # Mask all but the last position\n",
    "\n",
    "        # Forward pass\n",
    "        predicted_positions = model(images)\n",
    "\n",
    "        # Apply the mask\n",
    "        masked_predicted = predicted_positions * mask\n",
    "        masked_targets = targets * mask\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(masked_predicted, masked_targets)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sequence(model, sequence_of_images, device):\n",
    "    \"\"\"\n",
    "    Predict the positions for a given sequence of images.\n",
    "\n",
    "    :param model: Trained PyTorch model.\n",
    "    :param sequence_of_images: A sequence of images as a PyTorch tensor.\n",
    "    :param device: Device to perform computations on (e.g., 'cuda', 'cpu').\n",
    "    :return: Predicted positions as a tensor.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        sequence_of_images = sequence_of_images.to(device)\n",
    "        predicted_positions = model(sequence_of_images)\n",
    "    return predicted_positions.cpu()\n",
    "\n",
    "\n",
    "def create_images_from_positions(positions, image_size=(16, 16)):\n",
    "    \"\"\"\n",
    "    Create a sequence of PIL images from a sequence of position tuples.\n",
    "\n",
    "    :param positions: A sequence of (x, y) tuples.\n",
    "    :param image_size: Size of the output images (width, height).\n",
    "    :return: A list of PIL.Image.Image objects.\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    for pos in positions:\n",
    "        img = Image.new(\"L\", image_size, 255)  # Create a white image\n",
    "        draw = ImageDraw.Draw(img)\n",
    "        x, y = pos\n",
    "        draw.point((x, y), fill=0)  # Draw the black pixel\n",
    "        images.append(img)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x200 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABiYAAACnCAYAAACVZOSQAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAHfUlEQVR4nO3dMW4iSwBF0WqLFDtHZv8Ls8QCTO76kUc/GqCMLiNzTgyeCp7Kal31eJtzzgEAAAAAABB4efQBAAAAAACA5yFMAAAAAAAAGWECAAAAAADICBMAAAAAAEBGmAAAAAAAADLCBAAAAAAAkBEmAAAAAACAjDABAAAAAABkdqtf/Pr6GqfTaez3+7Ft2z3PxC825xzn83kcDofx8nJ9F7M3VqzubQybY407jpI7jpo7jpK9UbM5SvZGyXMDtWs3txwmTqfTOB6Pq1/nyX18fIz39/erP29v/MStexvD5vgZdxwldxw1dxwle6Nmc5TsjZLnBmqXNrccJvb7/Z9/4PX1dfXH8GQ+Pz/H8Xj8s59r2RsrVvc2hs2xxh1HyR1HzR1Hyd6o2Rwle6PkuYHatZtbDhPfr++8vr4aJje79fUve+MnVl43tDl+wh1HyR1HzR1Hyd6o2Rwle6PkuYHapc3549cAAAAAAEBGmAAAAAAAADLCBAAAAAAAkBEmAAAAAACAjDABAAAAAABkhAkAAAAAACAjTAAAAAAAABlhAgAAAAAAyAgTAAAAAABARpgAAAAAAAAywgQAAAAAAJARJgAAAAAAgIwwAQAAAAAAZIQJAAAAAAAgI0wAAAAAAAAZYQIAAAAAAMgIEwAAAAAAQEaYAAAAAAAAMsIEAAAAAACQESYAAAAAAICMMAEAAAAAAGSECQAAAAAAICNMAAAAAAAAGWECAAAAAADICBMAAAAAAEBGmAAAAAAAADLCBAAAAAAAkBEmAAAAAACAjDABAAAAAABkhAkAAAAAACAjTAAAAAAAABlhAgAAAAAAyAgTAAAAAABARpgAAAAAAAAywgQAAAAAAJARJgAAAAAAgIwwAQAAAAAAZIQJAAAAAAAgI0wAAAAAAAAZYQIAAAAAAMgIEwAAAAAAQEaYAAAAAAAAMsIEAAAAAACQESYAAAAAAICMMAEAAAAAAGSECQAAAAAAICNMAAAAAAAAGWECAAAAAADICBMAAAAAAEBGmAAAAAAAADLCBAAAAAAAkBEmAAAAAACAjDABAAAAAABkhAkAAAAAACAjTAAAAAAAABlhAgAAAAAAyAgTAAAAAABARpgAAAAAAAAywgQAAAAAAJARJgAAAAAAgIwwAQAAAAAAZIQJAAAAAAAgI0wAAAAAAAAZYQIAAAAAAMgIEwAAAAAAQEaYAAAAAAAAMsIEAAAAAACQ2T36APzdtm0XPzPnDE4CAAAAAAA/540JAAAAAAAgI0wAAAAAAAAZYQIAAAAAAMgIEwAAAAAAQEaYAAAAAAAAMsIEAAAAAACQESYAAAAAAICMMAEAAAAAAGR2jz4AfzfnfPQRAAAAAADgbrwxAQAAAAAAZIQJAAAAAAAgI0wAAAAAAAAZYQIAAAAAAMgIEwAAAAAAQEaYAAAAAAAAMsIEAAAAAACQESYAAAAAAICMMAEAAAAAAGSECQAAAAAAICNMAAAAAAAAGWECAAAAAADICBMAAAAAAEBGmAAAAAAAADLCBAAAAAAAkBEmAAAAAACAjDABAAAAAABkhAkAAAAAACAjTAAAAAAAABlhAgAAAAAAyAgTAAAAAABARpgAAAAAAAAywgQAAAAAAJARJgAAAAAAgIwwAQAAAAAAZIQJAAAAAAAgs3v0AYB/x7ZtFz8z5wxOAgAAAAD8Vt6YAAAAAAAAMsIEAAAAAACQESYAAAAAAICMMAEAAAAAAGSECQAAAAAAICNMAAAAAAAAGWECAAAAAADICBMAAAAAAEBm9+gDAP+OOeejjwAAAAAA/HLemAAAAAAAADLCBAAAAAAAkBEmAAAAAACAjDABAAAAAABkhAkAAAAAACAjTAAAAAAAABlhAgAAAAAAyAgTAAAAAABAZvfoAwDwO23bdvEzc87gJDwDe+Oe7ImSvXFPl/ZkS9ybzXEvfh9Ss7nH88YEAAAAAACQESYAAAAAAICMMAEAAAAAAGSECQAAAAAAICNMAAAAAAAAGWECAAAAAADICBMAAAAAAEBGmAAAAAAAADK7Rx8AgN9pzvnoI/BE7I17sidK9sY92RM1m+NebImazT2eNyYAAAAAAICMMAEAAAAAAGSECQAAAAAAICNMAAAAAAAAGWECAAAAAADICBMAAAAAAEBGmAAAAAAAADK71S/OOccYY3x+ft7tMPx+33v53s+17I0Vq3v7/3dsjlu44yi546i54yjZGzWbo2RvlDw3ULt2c8th4nw+jzHGOB6Pqz+CJ3Y+n8fb29tNnx/D3lhz696+vzOGzbHGHUfJHUfNHUfJ3qjZHCV7o+S5gdqlzW1zJZeNMb6+vsbpdBr7/X5s27Z8QJ7LnHOcz+dxOBzGy8v1/5OYvbFidW9j2Bxr3HGU3HHU3HGU7I2azVGyN0qeG6hdu7nlMAEAAAAAAHArf/waAAAAAADICBMAAAAAAEBGmAAAAAAAADLCBAAAAAAAkBEmAAAAAACAjDABAAAAAABkhAkAAAAAACAjTAAAAAAAABlhAgAAAAAAyAgTAAAAAABARpgAAAAAAAAywgQAAAAAAJD5D5EEarxC6NczAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 2000x200 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sequence, positions = generate_random_sequence()\n",
    "sequence_of_images = prepare_sequence_for_model(sequence)\n",
    "predicted_positions = predict_sequence(\n",
    "    model, sequence_of_images, device=\"cpu\"\n",
    ")  # or 'cuda'\n",
    "predicted_positions = predicted_positions[:, 1:, :]\n",
    "predicted_images = create_images_from_positions(predicted_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
