{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from data_generation import *\n",
    "from torchvision import transforms\n",
    "from PIL import Image, ImageDraw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x200 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(6.118723977510821, 5.050649633235524), (5.677215560876097, 1.6532339699126974), (5.235707144241373, 1.9665955923149168), (4.794198727606649, 1.4985884051982148), (4.3526903109719255, 1.498785460924016), (3.9111818943372016, 1.498785460924016), (3.469673477702478, 1.498785460924016), (3.028165061067754, 1.498785460924016), (2.58665664443303, 1.498785460924016), (2.1451482277983063, 1.498785460924016)]\n"
     ]
    }
   ],
   "source": [
    "sequence, positions = generate_random_sequence()\n",
    "display_sequence(sequence)\n",
    "print(positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "N = 1000  # Number of sequences\n",
    "L = 10  # Length of each sequence\n",
    "H, W = 16, 16  # Dimensions of the images\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "num_epochs = 20\n",
    "\n",
    "\n",
    "# Data Generation\n",
    "class PixelDataset(Dataset):\n",
    "    def __init__(self, num_sequences, sequence_length):\n",
    "        self.data = []\n",
    "        self.targets = []\n",
    "        for _ in range(num_sequences):\n",
    "            images, positions = generate_random_sequence()\n",
    "            self.data.append(images)  # Store the whole sequence\n",
    "            self.targets.append(positions)  # Store all positions\n",
    "\n",
    "        self.transform = transforms.ToTensor()\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Transform and stack images to create a sequence\n",
    "        sequence_of_images = [self.transform(image) for image in self.data[idx]]\n",
    "        sequence_of_images = torch.stack(sequence_of_images)\n",
    "\n",
    "        # Stack target positions\n",
    "        target_positions = torch.FloatTensor(self.targets[idx])\n",
    "\n",
    "        return sequence_of_images, target_positions\n",
    "\n",
    "\n",
    "# Creating Dataset and DataLoader\n",
    "dataset = PixelDataset(N, L)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 2.4393\n",
      "Epoch [2/20], Loss: 2.0480\n",
      "Epoch [3/20], Loss: 1.0963\n",
      "Epoch [4/20], Loss: 1.7283\n",
      "Epoch [5/20], Loss: 0.8799\n",
      "Epoch [6/20], Loss: 1.4094\n",
      "Epoch [7/20], Loss: 0.9298\n",
      "Epoch [8/20], Loss: 1.2095\n",
      "Epoch [9/20], Loss: 0.9576\n",
      "Epoch [10/20], Loss: 1.0218\n",
      "Epoch [11/20], Loss: 0.4336\n",
      "Epoch [12/20], Loss: 0.6249\n",
      "Epoch [13/20], Loss: 0.6877\n",
      "Epoch [14/20], Loss: 0.7807\n",
      "Epoch [15/20], Loss: 0.6369\n",
      "Epoch [16/20], Loss: 0.7651\n",
      "Epoch [17/20], Loss: 0.5060\n",
      "Epoch [18/20], Loss: 0.6236\n",
      "Epoch [19/20], Loss: 0.5601\n",
      "Epoch [20/20], Loss: 0.5454\n"
     ]
    }
   ],
   "source": [
    "class SimpleTransformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleTransformer, self).__init__()\n",
    "        self.encoder = nn.TransformerEncoderLayer(d_model=H * W, nhead=4)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder, num_layers=3)\n",
    "        self.fc = nn.Linear(H * W, 2)  # Output is x, y position\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Flatten the image and encode\n",
    "        batch_size, sequence_length, _, _, _ = x.size()\n",
    "        x = x.view(batch_size, sequence_length, -1)\n",
    "        encoded = self.transformer_encoder(x)\n",
    "        output = self.fc(encoded)  # Apply the linear layer to the entire sequence\n",
    "        return output\n",
    "\n",
    "\n",
    "def euclidean_distance_loss(output, target):\n",
    "    epsilon = 1e-6  # A small constant to avoid sqrt(0)\n",
    "    return torch.sqrt(torch.sum((output - target) ** 2, dim=-1) + epsilon).mean()\n",
    "\n",
    "\n",
    "model = SimpleTransformer()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = euclidean_distance_loss\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for images, positions in dataloader:\n",
    "        # Shift the positions by one to create the targets\n",
    "        targets = torch.roll(positions, -1, dims=1)\n",
    "\n",
    "        # Mask the last position in each sequence as it has no valid next position\n",
    "        mask = torch.zeros_like(targets)\n",
    "        mask[:, :-1, :] = 1  # Mask all but the last position\n",
    "\n",
    "        # Forward pass\n",
    "        predicted_positions = model(images)\n",
    "\n",
    "        # Apply the mask\n",
    "        masked_predicted = predicted_positions * mask\n",
    "        masked_targets = targets * mask\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(masked_predicted, masked_targets)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sequence(model, sequence_of_images, device):\n",
    "    \"\"\"\n",
    "    Predict the positions for a given sequence of images.\n",
    "\n",
    "    :param model: Trained PyTorch model.\n",
    "    :param sequence_of_images: A sequence of images as a PyTorch tensor.\n",
    "    :param device: Device to perform computations on (e.g., 'cuda', 'cpu').\n",
    "    :return: Predicted positions as a tensor.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        sequence_of_images = sequence_of_images.to(device)\n",
    "        predicted_positions = model(sequence_of_images)\n",
    "    return predicted_positions.cpu()\n",
    "\n",
    "\n",
    "def create_images_from_positions(positions, image_size=(16, 16)):\n",
    "    \"\"\"\n",
    "    Create a sequence of PIL images from a sequence of position tuples.\n",
    "    Includes an empty image at the beginning of the sequence.\n",
    "\n",
    "    :param positions: A sequence of (x, y) tuples as a PyTorch tensor.\n",
    "    :param image_size: Size of the output images (width, height).\n",
    "    :return: A list of PIL.Image.Image objects.\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    positions = positions.squeeze()\n",
    "    if positions.dim() == 1:\n",
    "        positions = positions.unsqueeze(0)\n",
    "\n",
    "    # Create and add an empty image for the first position\n",
    "    empty_img = Image.new(\"1\", image_size, 255)  # White image\n",
    "    images.append(empty_img)\n",
    "\n",
    "    for pos in positions:\n",
    "        img = Image.new(\"1\", image_size, 255)  # Create a white image\n",
    "        draw = ImageDraw.Draw(img)\n",
    "        x, y = pos.tolist()\n",
    "        y = image_size[1] - 1 - y  # Adjust y-coordinate\n",
    "        draw.point((x, y), fill=0)  # Draw the black pixel\n",
    "        images.append(img)\n",
    "\n",
    "    return images\n",
    "\n",
    "\n",
    "def prepare_sequence_for_model(image_sequence):\n",
    "    \"\"\"\n",
    "    Prepare a sequence of PIL images for input into the model.\n",
    "\n",
    "    :param image_sequence: A list of PIL.Image.Image objects.\n",
    "    :return: A PyTorch tensor representing the sequence.\n",
    "    \"\"\"\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Grayscale(),  # Convert to grayscale if not already\n",
    "            transforms.ToTensor(),  # Convert PIL Image to PyTorch tensor\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    tensor_sequence = [transform(img) for img in image_sequence]\n",
    "    tensor_sequence = torch.stack(tensor_sequence)  # Stack to create a single tensor\n",
    "    tensor_sequence = tensor_sequence.unsqueeze(0)  # Add a batch dimension\n",
    "\n",
    "    return tensor_sequence\n",
    "\n",
    "\n",
    "def overlay_predicted_positions(s1, s2):\n",
    "    \"\"\"\n",
    "    s1 black pixels stay black\n",
    "    s2 black pixels become blue\n",
    "\n",
    "    :param s1: A list of PIL.Image.Image objects representing the original sequence.\n",
    "    :param s2: A list of PIL.Image.Image objects representing the predicted sequence.\n",
    "    :return: A list of PIL.Image.Image objects with overlaid positions.\n",
    "    \"\"\"\n",
    "    overlaid_images = []\n",
    "    for img1, img2 in zip(s1, s2):\n",
    "        img1 = img1.convert(\"RGB\")  # Convert to RGB\n",
    "        img2 = img2.convert(\"RGB\")\n",
    "\n",
    "        # Create a new image for the overlay\n",
    "        new_img = Image.new(\"RGB\", img1.size)\n",
    "        pixels1 = img1.load()\n",
    "        pixels2 = img2.load()\n",
    "\n",
    "        for x in range(img1.width):\n",
    "            for y in range(img1.height):\n",
    "                if pixels2[x, y] == (0, 0, 0):  # Black pixel in s2\n",
    "                    new_img.putpixel((x, y), (0, 0, 255))  # Make it blue\n",
    "                else:\n",
    "                    new_img.putpixel((x, y), pixels1[x, y])  # Use pixel from s1\n",
    "\n",
    "        overlaid_images.append(new_img)\n",
    "\n",
    "    return overlaid_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x200 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sequence, positions = generate_random_sequence()\n",
    "sequence_of_images = prepare_sequence_for_model(sequence)\n",
    "predicted_positions = predict_sequence(\n",
    "    model, sequence_of_images, device=\"cpu\"\n",
    ")  # or 'cuda'\n",
    "predicted_positions = predicted_positions[:, 1:, :]\n",
    "predicted_images = create_images_from_positions(predicted_positions)\n",
    "display_sequence(overlay_predicted_positions(sequence, predicted_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
